{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a98b697-e6e4-4528-bb8e-5e42e36313cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.cores': '2', 'spark.executor.memory': '4g', 'spark.driver.memory': '4g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>744</td><td>application_1761923966900_0756</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0756/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-160.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0756_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>748</td><td>application_1761923966900_0760</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0760/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0760_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>758</td><td>application_1761923966900_0770</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0770/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-11.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0770_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>761</td><td>application_1761923966900_0773</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0773/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-146.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0773_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>803</td><td>application_1761923966900_0815</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0815/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-189.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0815_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>814</td><td>application_1761923966900_0826</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0826/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-65.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0826_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>825</td><td>application_1761923966900_0837</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0837/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-212.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0837_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>832</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \"spark.executor.cores\": \"2\",\n",
    "    \"spark.executor.memory\": \"4g\",\n",
    "    \"spark.driver.memory\": \"4g\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd03aac5-cf8c-4a44-88e1-be62718fa708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 833 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "def run_query_4():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import (\n",
    "        col, count, avg, round as spark_round,\n",
    "        radians, sin, cos, sqrt, asin\n",
    "    )\n",
    "    from pyspark.sql.functions import min as spark_min\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Load and prepare police stations\n",
    "    stations = spark.read.option(\"header\", True).csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\"\n",
    "    ).withColumnRenamed(\"DIVISION\", \"division\") \\\n",
    "     .withColumnRenamed(\"X\", \"station_lon\") \\\n",
    "     .withColumnRenamed(\"Y\", \"station_lat\") \\\n",
    "     .withColumn(\"station_lon\", col(\"station_lon\").cast(\"double\")) \\\n",
    "     .withColumn(\"station_lat\", col(\"station_lat\").cast(\"double\"))\n",
    "\n",
    "    # Load crime data\n",
    "    crime_2010_2019 = spark.read.option(\"header\", True).csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "    )\n",
    "    crime_2020_2025 = spark.read.option(\"header\", True).csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "    )\n",
    "    combined_crime = crime_2010_2019.unionByName(crime_2020_2025)\n",
    "\n",
    "    # Filter Null Island points early\n",
    "    crime_coords = combined_crime.select(\n",
    "        col(\"DR_NO\"),\n",
    "        col(\"LAT\").cast(\"double\").alias(\"crime_lat\"),\n",
    "        col(\"LON\").cast(\"double\").alias(\"crime_lon\")\n",
    "    ).filter(\n",
    "        (col(\"crime_lat\").isNotNull()) &\n",
    "        (col(\"crime_lon\").isNotNull()) &\n",
    "        ~((col(\"crime_lat\") == 0) & (col(\"crime_lon\") == 0))\n",
    "    )\n",
    "\n",
    "    # Haversine distance using Spark SQL expressions\n",
    "    def haversine_expr(lat1, lon1, lat2, lon2):\n",
    "        return 2 * 6371.0 * asin(\n",
    "            sqrt(\n",
    "                sin((radians(lat2) - radians(lat1)) / 2) ** 2 +\n",
    "                cos(radians(lat1)) * cos(radians(lat2)) *\n",
    "                sin((radians(lon2) - radians(lon1)) / 2) ** 2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Compute distances (join type chosen by optimizer)\n",
    "    crime_station = crime_coords.crossJoin(stations) \\\n",
    "        .withColumn(\n",
    "            \"distance\",\n",
    "            haversine_expr(\n",
    "                col(\"crime_lat\"), col(\"crime_lon\"),\n",
    "                col(\"station_lat\"), col(\"station_lon\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Find nearest station\n",
    "    crime_min = crime_station.groupBy(\"DR_NO\") \\\n",
    "        .agg(spark_min(\"distance\").alias(\"min_distance\"))\n",
    "\n",
    "    crime_nearest = crime_min.join(\n",
    "        crime_station,\n",
    "        (crime_min.DR_NO == crime_station.DR_NO) &\n",
    "        (crime_min.min_distance == crime_station.distance),\n",
    "        \"inner\"\n",
    "    ).select(\"division\", \"distance\")\n",
    "\n",
    "    # Division-level stats\n",
    "    division_stats = crime_nearest.groupBy(\"division\") \\\n",
    "        .agg(\n",
    "            spark_round(avg(\"distance\"), 3).alias(\"average_distance\"),\n",
    "            count(\"*\").alias(\"#\")\n",
    "        ).select(\"division\", \"average_distance\", \"#\") \\\n",
    "         .orderBy(col(\"#\").desc())\n",
    "\n",
    "    return division_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1123fa7-590f-4493-b673-7f8f4c14a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 833 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "df1 = run_query_4()\n",
    "\n",
    "df1.show(50,truncate=False)\n",
    "df1.explain(mode=\"extended\")  \n",
    "end_time = time.time()\n",
    "print(f\"Execution time for run_query_4: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990d540-ade3-4021-b51c-e94957868470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
