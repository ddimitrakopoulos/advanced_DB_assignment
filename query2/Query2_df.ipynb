{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0335c512-eee3-42fb-9bf3-609b17dd5978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.cores': '1', 'spark.executor.memory': '2g', 'spark.driver.memory': '4g', 'spark.sql.shuffle.partitions': '8'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>664</td><td>application_1765289937462_0657</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0657/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0657_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>666</td><td>application_1765289937462_0659</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0659/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0659_01_000004/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>667</td><td>application_1765289937462_0660</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0660/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0660_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>668</td><td>application_1765289937462_0661</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0661/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0661_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>669</td><td>application_1765289937462_0662</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0662/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0662_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>671</td><td>application_1765289937462_0664</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0664/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0664_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>672</td><td>application_1765289937462_0665</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0665/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0665_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>673</td><td>application_1765289937462_0666</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0666/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0666_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>690</td><td>application_1765289937462_0683</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0683/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0683_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>691</td><td>application_1765289937462_0684</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0684/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0684_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>692</td><td>application_1765289937462_0685</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0685/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0685_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>693</td><td>application_1765289937462_0686</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0686/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-103.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0686_01_000002/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.executor.instances\": \"4\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.executor.memory\": \"2g\",\n",
    "    \"spark.driver.memory\": \"4g\",\n",
    "    \"spark.sql.shuffle.partitions\": \"8\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d418e8b-184c-42e1-8cd5-fee82da4ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import (\n",
    "    col, substring, sum as F_sum, round as F_round, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Paths\n",
    "crime_2010_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "crime_2020_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "re_codes_path   = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\"\n",
    "\n",
    "# Schema για crime data (όπως στο Query 1)\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\",          StringType()),\n",
    "    StructField(\"Date Rptd\",      StringType()),\n",
    "    StructField(\"DATE OCC\",       StringType()),\n",
    "    StructField(\"TIME OCC\",       StringType()),\n",
    "    StructField(\"AREA\",           StringType()),\n",
    "    StructField(\"AREA NAME\",      StringType()),\n",
    "    StructField(\"Rpt Dist No\",    StringType()),\n",
    "    StructField(\"Part 1-2\",       StringType()),\n",
    "    StructField(\"Crm Cd\",         StringType()),\n",
    "    StructField(\"Crm Cd Desc\",    StringType()),\n",
    "    StructField(\"Mocodes\",        StringType()),\n",
    "    StructField(\"Vict Age\",       IntegerType()),\n",
    "    StructField(\"Vict Sex\",       StringType()),\n",
    "    StructField(\"Vict Descent\",   StringType()),\n",
    "    StructField(\"Premis Cd\",      StringType()),\n",
    "    StructField(\"Premis Desc\",    StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\",    StringType()),\n",
    "    StructField(\"Status\",         StringType()),\n",
    "    StructField(\"Status Desc\",    StringType()),\n",
    "    StructField(\"Crm Cd 1\",       StringType()),\n",
    "    StructField(\"Crm Cd 2\",       StringType()),\n",
    "    StructField(\"Crm Cd 3\",       StringType()),\n",
    "    StructField(\"Crm Cd 4\",       StringType()),\n",
    "    StructField(\"LOCATION\",       StringType()),\n",
    "    StructField(\"Cross Street\",   StringType()),\n",
    "    StructField(\"LAT\",            DoubleType()),\n",
    "    StructField(\"LON\",            DoubleType())\n",
    "])\n",
    "\n",
    "# Schema για RE_codes\n",
    "re_codes_schema = StructType([\n",
    "    StructField(\"Vict Descent\",      StringType()),\n",
    "    StructField(\"Vict Descent Full\", StringType())\n",
    "])\n",
    "\n",
    "# Implementation 1: DataFrame API for Query 2\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"====================================================================\")\n",
    "print(\"Loading crime data & RE codes as DataFrames...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Διαβάζουμε crime data με explicit schema\n",
    "crime_2010_df = spark.read.format(\"csv\") \\\n",
    "    .options(header=\"true\") \\\n",
    "    .schema(crime_schema) \\\n",
    "    .load(crime_2010_path)\n",
    "\n",
    "crime_2020_df = spark.read.format(\"csv\") \\\n",
    "    .options(header=\"true\") \\\n",
    "    .schema(crime_schema) \\\n",
    "    .load(crime_2020_path)\n",
    "\n",
    "crime_df = crime_2010_df.unionByName(crime_2020_df)\n",
    "\n",
    "# Διαβάζουμε RE_codes (φυλετικά γκρουπ)\n",
    "re_codes_df = spark.read.format(\"csv\") \\\n",
    "    .options(header=\"true\") \\\n",
    "    .schema(re_codes_schema) \\\n",
    "    .load(re_codes_path)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"[INFO] Data loading + union: {t1 - t0:.2f} sec\")\n",
    "#crime_df.show(3, truncate=False)\n",
    "#re_codes_df.show(5, truncate=False)\n",
    "\n",
    "print(\"====================================================================\")\n",
    "print(\"DF pipeline for Query 2 (per year, top-3 Vict Descent)...\")\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "# 1) Εξάγουμε year από DATE OCC (πρώτα 4 ψηφία) και κρατάμε μόνο όσες έχουν Vict Descent\n",
    "crime_with_year_df = crime_df \\\n",
    "    .withColumn(\"year\", substring(col(\"DATE OCC\"), 1, 4).cast(\"int\")) \\\n",
    "    .filter(col(\"year\").isNotNull()) \\\n",
    "    .filter(col(\"Vict Descent\").isNotNull() & (col(\"Vict Descent\") != \"\"))\n",
    "\n",
    "# 2) Group by (year, Vict Descent) και count victims\n",
    "year_descent_counts_df = crime_with_year_df \\\n",
    "    .groupBy(\"year\", \"Vict Descent\") \\\n",
    "    .count()\n",
    "\n",
    "# 3) Σύνολο θυμάτων ανά έτος\n",
    "year_totals_df = year_descent_counts_df \\\n",
    "    .groupBy(\"year\") \\\n",
    "    .agg(F_sum(\"count\").alias(\"total_victims\"))\n",
    "\n",
    "# 4) Join για να υπολογίσουμε ποσοστά\n",
    "with_totals_df = year_descent_counts_df.join(year_totals_df, on=\"year\")\n",
    "\n",
    "with_percent_df = with_totals_df \\\n",
    "    .withColumn(\"percentage\", (col(\"count\") / col(\"total_victims\")) * 100.0)\n",
    "\n",
    "# 5) Join με RE_codes για να πάρουμε full περιγραφή (Vict Descent Full)\n",
    "with_labels_df = with_percent_df.join(\n",
    "    re_codes_df,\n",
    "    on=\"Vict Descent\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 6) Top-3 φυλετικά groups ανά έτος (Window + row_number)\n",
    "w = Window.partitionBy(\"year\").orderBy(col(\"count\").desc())\n",
    "\n",
    "ranked_df = with_labels_df \\\n",
    "    .withColumn(\"rn\", row_number().over(w))\n",
    "\n",
    "top3_per_year_df = ranked_df \\\n",
    "    .filter(col(\"rn\") <= 3) \\\n",
    "    .select(\n",
    "        col(\"year\"),\n",
    "        col(\"Vict Descent Full\").alias(\"Victim_Descent\"),\n",
    "        col(\"count\").alias(\"num_victims\"),\n",
    "        F_round(col(\"percentage\"), 2).alias(\"percentage\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"year\"), col(\"num_victims\").desc())\n",
    "\n",
    "t3 = time.time()\n",
    "\n",
    "top3_per_year_df.show(60, truncate=False)\n",
    "print(f\"[TIMING] DF Query 2 pipeline: {t3 - t2:.2f} sec\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
